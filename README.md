# Optimization Algorithms
In this project, I have implemented the following optimization algorithms from scratch with 3D plots.

1) Simple Gradient Descent
2) Momentum
3) Nesterov Accelerated Gradient (NAG)
4) Adaptive Gradient (AdaGrad)
5) Root Mean Squared Propagation (RMSprop)
6) Adaptive Moment Estimation (Adam)
(These can also be used with learning rate decay)

These optimizations algorithms are applied on -

1) Convex function (linear regression) - Optimization of a simple 1D linear regression problem (Linear regression problems have a convex cost function). The program, in this case, also allows the above optimization techniques to be paired with batch, mini-batch, or stochastic gradient descent.

2) Non-convex function (Styblinski Tang function) - Optimization of a function with multiple minima.

Content-

- on_linear_regression.ipynb - Optimization algorithms on linear regression.
- on_function.ipynb - Optimization algorithms on non-convex function.
- plot_img - folder used by the program files
- model_plots - 3D plot images of the above-mentioned optimization techniques.
